{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Explainable Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of      age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0     63    1   3       145   233    1        0      150      0      2.3   \n",
       "1     37    1   2       130   250    0        1      187      0      3.5   \n",
       "2     41    0   1       130   204    0        0      172      0      1.4   \n",
       "3     56    1   1       120   236    0        1      178      0      0.8   \n",
       "4     57    0   0       120   354    0        1      163      1      0.6   \n",
       "..   ...  ...  ..       ...   ...  ...      ...      ...    ...      ...   \n",
       "298   57    0   0       140   241    0        1      123      1      0.2   \n",
       "299   45    1   3       110   264    0        1      132      0      1.2   \n",
       "300   68    1   0       144   193    1        1      141      0      3.4   \n",
       "301   57    1   0       130   131    0        1      115      1      1.2   \n",
       "302   57    0   1       130   236    0        0      174      0      0.0   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "0        0   0     1       1  \n",
       "1        0   0     2       1  \n",
       "2        2   0     2       1  \n",
       "3        2   0     2       1  \n",
       "4        2   0     2       1  \n",
       "..     ...  ..   ...     ...  \n",
       "298      1   0     3       0  \n",
       "299      1   0     3       0  \n",
       "300      1   2     3       0  \n",
       "301      1   1     3       0  \n",
       "302      1   1     2       0  \n",
       "\n",
       "[303 rows x 14 columns]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\isabe\\\\Documents\\\\AI studies\\\\6.Semester\\\\Bachelor Thesis\\\\Paradime-NAMs\\\\Datasets\\\\heart.csv')\n",
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         0\n",
       "sex         0\n",
       "cp          0\n",
       "trestbps    0\n",
       "chol        0\n",
       "fbs         0\n",
       "restecg     0\n",
       "thalach     0\n",
       "exang       0\n",
       "oldpeak     0\n",
       "slope       0\n",
       "ca          0\n",
       "thal        0\n",
       "target      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       303 non-null    int64  \n",
      " 1   sex       303 non-null    int64  \n",
      " 2   cp        303 non-null    int64  \n",
      " 3   trestbps  303 non-null    int64  \n",
      " 4   chol      303 non-null    int64  \n",
      " 5   fbs       303 non-null    int64  \n",
      " 6   restecg   303 non-null    int64  \n",
      " 7   thalach   303 non-null    int64  \n",
      " 8   exang     303 non-null    int64  \n",
      " 9   oldpeak   303 non-null    float64\n",
      " 10  slope     303 non-null    int64  \n",
      " 11  ca        303 non-null    int64  \n",
      " 12  thal      303 non-null    int64  \n",
      " 13  target    303 non-null    int64  \n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 33.3 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(df.columns) - 1 # minus target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df):\n",
    "    # Iterate over each column in the DataFrame\n",
    "    for column in df.columns:\n",
    "        if len(df[column].unique()) == 2 and df[column].dtype == object:\n",
    "            # Binary labels\n",
    "            unique_labels = df[column].unique()\n",
    "            mapping = {unique_labels[0]: 0, unique_labels[1]: 1}\n",
    "            df[column] = df[column].map(mapping)\n",
    "        elif df[column].dtype == object:\n",
    "            # Multiple labels (strings)\n",
    "            dummies = pd.get_dummies(df[column], prefix=column)\n",
    "            df = pd.concat([df, dummies], axis=1)\n",
    "            df.drop(column, axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>110</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0     63    1   3       145   233    1        0      150      0      2.3   \n",
       "1     37    1   2       130   250    0        1      187      0      3.5   \n",
       "2     41    0   1       130   204    0        0      172      0      1.4   \n",
       "3     56    1   1       120   236    0        1      178      0      0.8   \n",
       "4     57    0   0       120   354    0        1      163      1      0.6   \n",
       "..   ...  ...  ..       ...   ...  ...      ...      ...    ...      ...   \n",
       "298   57    0   0       140   241    0        1      123      1      0.2   \n",
       "299   45    1   3       110   264    0        1      132      0      1.2   \n",
       "300   68    1   0       144   193    1        1      141      0      3.4   \n",
       "301   57    1   0       130   131    0        1      115      1      1.2   \n",
       "302   57    0   1       130   236    0        0      174      0      0.0   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "0        0   0     1       1  \n",
       "1        0   0     2       1  \n",
       "2        2   0     2       1  \n",
       "3        2   0     2       1  \n",
       "4        2   0     2       1  \n",
       "..     ...  ..   ...     ...  \n",
       "298      1   0     3       0  \n",
       "299      1   0     3       0  \n",
       "300      1   2     3       0  \n",
       "301      1   1     3       0  \n",
       "302      1   1     2       0  \n",
       "\n",
       "[303 rows x 14 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data = preprocess_dataframe(df)\n",
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting feature vector and target variable\n",
    "target_column = 'target' # should be specified by user \n",
    "X = df.drop([target_column], axis=1)\n",
    "y = df[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(242, 13) (61, 13)\n"
     ]
    }
   ],
   "source": [
    "# Training and testing data split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# check the shape of X_train and X_test\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the input ranges for each feature\n",
    "input_ranges = np.zeros((X.shape[1], 2))\n",
    "for i, col in enumerate(X.columns):\n",
    "    input_ranges[i, 0] = X[col].min()\n",
    "    input_ranges[i, 1] = X[col].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "class NAM(th.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1, num_layers=1):\n",
    "        super(NAM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.submodules = th.nn.ModuleList()\n",
    "        print(input_dim)\n",
    "\n",
    "        # Create the submodules for each input feature\n",
    "        for i in range(input_dim):\n",
    "            submodule = th.nn.Sequential()\n",
    "            # Add layers to the submodule\n",
    "            for l in range(num_layers):\n",
    "                if l == 0:\n",
    "                    submodule.add_module(f\"linear_{l}\", th.nn.Linear(1, hidden_dim))\n",
    "                else:\n",
    "                    submodule.add_module(f\"linear_{l}\", th.nn.Linear(hidden_dim, hidden_dim))\n",
    "                submodule.add_module(f\"ELU_{l}\", th.nn.ELU())\n",
    "                submodule.add_module(f\"dropout_{l}\", th.nn.Dropout(0.5))\n",
    "\n",
    "            # Add the output layer\n",
    "            submodule.add_module(f\"linear_{num_layers}\", th.nn.Linear(hidden_dim, output_dim))\n",
    "            self.submodules.append(submodule)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = th.zeros(x.shape[0], self.output_dim)\n",
    "\n",
    "        # Apply each submodule to its corresponding input feature and sum the results\n",
    "        for i in range(self.input_dim):\n",
    "            output += self.submodules[i](x[:, i].unsqueeze(1))\n",
    "\n",
    "        # Apply sigmoid activation for binary classification\n",
    "        return th.sigmoid(output)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        # Initialize the weights of linear layers\n",
    "        if type(m) == th.nn.Linear:\n",
    "            th.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def get_feature_maps(self, resolution=100):\n",
    "        # Initialize the output\n",
    "        output = th.zeros(self.input_dim, resolution)\n",
    "\n",
    "        # For each input dimension, pass it through the corresponding submodule\n",
    "        for i in range(self.input_dim):\n",
    "            for j in range(resolution):\n",
    "                # Add each input dimension output to the overall output\n",
    "                output[i, j] = self.submodules[i](th.tensor([[j / resolution]]))\n",
    "\n",
    "        # Return the overall output as a NumPy array\n",
    "        return output.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "NAM(\n",
      "  (submodules): ModuleList(\n",
      "    (0-12): 13 x Sequential(\n",
      "      (linear_0): Linear(in_features=1, out_features=10, bias=True)\n",
      "      (ELU_0): ELU(alpha=1.0)\n",
      "      (dropout_0): Dropout(p=0.5, inplace=False)\n",
      "      (linear_1): Linear(in_features=10, out_features=10, bias=True)\n",
      "      (ELU_1): ELU(alpha=1.0)\n",
      "      (dropout_1): Dropout(p=0.5, inplace=False)\n",
      "      (linear_2): Linear(in_features=10, out_features=10, bias=True)\n",
      "      (ELU_2): ELU(alpha=1.0)\n",
      "      (dropout_2): Dropout(p=0.5, inplace=False)\n",
      "      (linear_3): Linear(in_features=10, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the Neural Additive Model\n",
    "# One input dimension for each feature\n",
    "# One output dimension for binary classification, using sigmoid activation on the output layer\n",
    "\n",
    "model = NAM(input_dim, hidden_dim=10, output_dim=1, num_layers=3)\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "seed = 42\n",
    "th.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Initialize weights\n",
    "model.apply(model.init_weights)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Define the loss function (binary cross entropy for binary classification)\n",
    "loss_fn = th.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train function\n",
    "def train(model, X, y, optimizer, loss_fn):\n",
    "    model.train()  # Set the model to training mode\n",
    "    optimizer.zero_grad()  # Zero the gradients\n",
    "    X_tensor = th.tensor(X.to_numpy(), dtype=th.float32)  # Convert input data to tensor\n",
    "    y_tensor = th.tensor(y.to_numpy(), dtype=th.float32).unsqueeze(1)  # Convert target data to tensor\n",
    "    output = model(X_tensor)  # Forward pass to get the model output\n",
    "    loss = loss_fn(output, y_tensor)  # Compute the loss using the loss function\n",
    "    loss.backward()  # Compute the gradients through backpropagation\n",
    "    optimizer.step()  # Update the model weights using the optimizer\n",
    "    return loss.item()  # Return the loss value\n",
    "# Define the evaluation function\n",
    "def evaluate(model, X, y, loss_fn):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    X_tensor = th.tensor(X.to_numpy(), dtype=th.float32)  # Convert input data to tensor\n",
    "    y_tensor = th.tensor(y.to_numpy(), dtype=th.float32).unsqueeze(1)  # Convert target data to tensor\n",
    "    output = model(X_tensor)  # Forward pass to get the model output\n",
    "    loss = loss_fn(output, y_tensor)  # Compute the loss using the loss function\n",
    "    predictions = (output > 0.5).float()  # Threshold the output probabilities to binary predictions\n",
    "    correct = (predictions == y_tensor).sum().item()  # Count the number of correct predictions\n",
    "    accuracy = correct / y_tensor.shape[0]  # Compute the accuracy\n",
    "    return loss.item(), accuracy  # Return the loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss = 1.0273, test loss = 0.8726, test accuracy = 0.4590\n",
      "Epoch 2: train loss = 1.0675, test loss = 0.8248, test accuracy = 0.4754\n",
      "Epoch 3: train loss = 0.9585, test loss = 0.7865, test accuracy = 0.4754\n",
      "Epoch 4: train loss = 0.9115, test loss = 0.7567, test accuracy = 0.5410\n",
      "Epoch 5: train loss = 0.9313, test loss = 0.7349, test accuracy = 0.4590\n",
      "Epoch 6: train loss = 0.9627, test loss = 0.7192, test accuracy = 0.5574\n",
      "Epoch 7: train loss = 0.9740, test loss = 0.7085, test accuracy = 0.5574\n",
      "Epoch 8: train loss = 0.8977, test loss = 0.7012, test accuracy = 0.5574\n",
      "Epoch 9: train loss = 0.8935, test loss = 0.6959, test accuracy = 0.5246\n",
      "Epoch 10: train loss = 1.0353, test loss = 0.6916, test accuracy = 0.5082\n",
      "Epoch 11: train loss = 0.8622, test loss = 0.6873, test accuracy = 0.5082\n",
      "Epoch 12: train loss = 0.9035, test loss = 0.6828, test accuracy = 0.5082\n",
      "Epoch 13: train loss = 0.8589, test loss = 0.6776, test accuracy = 0.5246\n",
      "Epoch 14: train loss = 0.9162, test loss = 0.6722, test accuracy = 0.5410\n",
      "Epoch 15: train loss = 0.9005, test loss = 0.6666, test accuracy = 0.5738\n",
      "Epoch 16: train loss = 0.8447, test loss = 0.6607, test accuracy = 0.5738\n",
      "Epoch 17: train loss = 0.9435, test loss = 0.6548, test accuracy = 0.5902\n",
      "Epoch 18: train loss = 0.8161, test loss = 0.6491, test accuracy = 0.5738\n",
      "Epoch 19: train loss = 0.8601, test loss = 0.6439, test accuracy = 0.6066\n",
      "Epoch 20: train loss = 0.8461, test loss = 0.6393, test accuracy = 0.6066\n",
      "Epoch 21: train loss = 0.8577, test loss = 0.6354, test accuracy = 0.6230\n",
      "Epoch 22: train loss = 0.9320, test loss = 0.6319, test accuracy = 0.6393\n",
      "Epoch 23: train loss = 0.8284, test loss = 0.6289, test accuracy = 0.6557\n",
      "Epoch 24: train loss = 0.7469, test loss = 0.6264, test accuracy = 0.6885\n",
      "Epoch 25: train loss = 0.8532, test loss = 0.6242, test accuracy = 0.6721\n",
      "Epoch 26: train loss = 0.8250, test loss = 0.6220, test accuracy = 0.6721\n",
      "Epoch 27: train loss = 0.8176, test loss = 0.6202, test accuracy = 0.6885\n",
      "Epoch 28: train loss = 0.8074, test loss = 0.6179, test accuracy = 0.6721\n",
      "Epoch 29: train loss = 0.8354, test loss = 0.6149, test accuracy = 0.6721\n",
      "Epoch 30: train loss = 0.7795, test loss = 0.6111, test accuracy = 0.6721\n",
      "Epoch 31: train loss = 0.7618, test loss = 0.6074, test accuracy = 0.6885\n",
      "Epoch 32: train loss = 0.8574, test loss = 0.6035, test accuracy = 0.6885\n",
      "Epoch 33: train loss = 0.7508, test loss = 0.5992, test accuracy = 0.7049\n",
      "Epoch 34: train loss = 0.7267, test loss = 0.5951, test accuracy = 0.6885\n",
      "Epoch 35: train loss = 0.7426, test loss = 0.5905, test accuracy = 0.7049\n",
      "Epoch 36: train loss = 0.7817, test loss = 0.5858, test accuracy = 0.7049\n",
      "Epoch 37: train loss = 0.7043, test loss = 0.5813, test accuracy = 0.7049\n",
      "Epoch 38: train loss = 0.8061, test loss = 0.5766, test accuracy = 0.7541\n",
      "Epoch 39: train loss = 0.7227, test loss = 0.5722, test accuracy = 0.7541\n",
      "Epoch 40: train loss = 0.6758, test loss = 0.5678, test accuracy = 0.7705\n",
      "Epoch 41: train loss = 0.7976, test loss = 0.5637, test accuracy = 0.7541\n",
      "Epoch 42: train loss = 0.7169, test loss = 0.5597, test accuracy = 0.7541\n",
      "Epoch 43: train loss = 0.7830, test loss = 0.5561, test accuracy = 0.7541\n",
      "Epoch 44: train loss = 0.7314, test loss = 0.5527, test accuracy = 0.7541\n",
      "Epoch 45: train loss = 0.6421, test loss = 0.5494, test accuracy = 0.7705\n",
      "Epoch 46: train loss = 0.8132, test loss = 0.5465, test accuracy = 0.7705\n",
      "Epoch 47: train loss = 0.7153, test loss = 0.5439, test accuracy = 0.7705\n",
      "Epoch 48: train loss = 0.6599, test loss = 0.5413, test accuracy = 0.7705\n",
      "Epoch 49: train loss = 0.6527, test loss = 0.5389, test accuracy = 0.7705\n",
      "Epoch 50: train loss = 0.6435, test loss = 0.5367, test accuracy = 0.7705\n",
      "Epoch 51: train loss = 0.7189, test loss = 0.5345, test accuracy = 0.7705\n",
      "Epoch 52: train loss = 0.7010, test loss = 0.5326, test accuracy = 0.7705\n",
      "Epoch 53: train loss = 0.6563, test loss = 0.5307, test accuracy = 0.7705\n",
      "Epoch 54: train loss = 0.7233, test loss = 0.5289, test accuracy = 0.7705\n",
      "Epoch 55: train loss = 0.7224, test loss = 0.5271, test accuracy = 0.7541\n",
      "Epoch 56: train loss = 0.6775, test loss = 0.5253, test accuracy = 0.7541\n",
      "Epoch 57: train loss = 0.7004, test loss = 0.5236, test accuracy = 0.7541\n",
      "Epoch 58: train loss = 0.6530, test loss = 0.5219, test accuracy = 0.7541\n",
      "Epoch 59: train loss = 0.6552, test loss = 0.5200, test accuracy = 0.7541\n",
      "Epoch 60: train loss = 0.6357, test loss = 0.5182, test accuracy = 0.7541\n",
      "Epoch 61: train loss = 0.6514, test loss = 0.5163, test accuracy = 0.7541\n",
      "Epoch 62: train loss = 0.6991, test loss = 0.5143, test accuracy = 0.7705\n",
      "Epoch 63: train loss = 0.5950, test loss = 0.5124, test accuracy = 0.7705\n",
      "Epoch 64: train loss = 0.6645, test loss = 0.5105, test accuracy = 0.7705\n",
      "Epoch 65: train loss = 0.6685, test loss = 0.5086, test accuracy = 0.7705\n",
      "Epoch 66: train loss = 0.6178, test loss = 0.5067, test accuracy = 0.7705\n",
      "Epoch 67: train loss = 0.6161, test loss = 0.5048, test accuracy = 0.7705\n",
      "Epoch 68: train loss = 0.6408, test loss = 0.5028, test accuracy = 0.7705\n",
      "Epoch 69: train loss = 0.5978, test loss = 0.5008, test accuracy = 0.7705\n",
      "Epoch 70: train loss = 0.6302, test loss = 0.4990, test accuracy = 0.7705\n",
      "Epoch 71: train loss = 0.6231, test loss = 0.4972, test accuracy = 0.7705\n",
      "Epoch 72: train loss = 0.6894, test loss = 0.4957, test accuracy = 0.7705\n",
      "Epoch 73: train loss = 0.6143, test loss = 0.4943, test accuracy = 0.7705\n",
      "Epoch 74: train loss = 0.5617, test loss = 0.4928, test accuracy = 0.7705\n",
      "Epoch 75: train loss = 0.6815, test loss = 0.4914, test accuracy = 0.7705\n",
      "Epoch 76: train loss = 0.5891, test loss = 0.4900, test accuracy = 0.7705\n",
      "Epoch 77: train loss = 0.6488, test loss = 0.4887, test accuracy = 0.7705\n",
      "Epoch 78: train loss = 0.6419, test loss = 0.4874, test accuracy = 0.7705\n",
      "Epoch 79: train loss = 0.6001, test loss = 0.4862, test accuracy = 0.7705\n",
      "Epoch 80: train loss = 0.6353, test loss = 0.4851, test accuracy = 0.7705\n",
      "Epoch 81: train loss = 0.5698, test loss = 0.4839, test accuracy = 0.7705\n",
      "Epoch 82: train loss = 0.6518, test loss = 0.4828, test accuracy = 0.7705\n",
      "Epoch 83: train loss = 0.5892, test loss = 0.4817, test accuracy = 0.7705\n",
      "Epoch 84: train loss = 0.5924, test loss = 0.4807, test accuracy = 0.7705\n",
      "Epoch 85: train loss = 0.6157, test loss = 0.4797, test accuracy = 0.7705\n",
      "Epoch 86: train loss = 0.5401, test loss = 0.4787, test accuracy = 0.7705\n",
      "Epoch 87: train loss = 0.6222, test loss = 0.4776, test accuracy = 0.7705\n",
      "Epoch 88: train loss = 0.6090, test loss = 0.4768, test accuracy = 0.7705\n",
      "Epoch 89: train loss = 0.6405, test loss = 0.4760, test accuracy = 0.7541\n",
      "Epoch 90: train loss = 0.6337, test loss = 0.4752, test accuracy = 0.7541\n",
      "Epoch 91: train loss = 0.5509, test loss = 0.4745, test accuracy = 0.7541\n",
      "Epoch 92: train loss = 0.5527, test loss = 0.4739, test accuracy = 0.7541\n",
      "Epoch 93: train loss = 0.6513, test loss = 0.4733, test accuracy = 0.7541\n",
      "Epoch 94: train loss = 0.5428, test loss = 0.4727, test accuracy = 0.7541\n",
      "Epoch 95: train loss = 0.5471, test loss = 0.4720, test accuracy = 0.7541\n",
      "Epoch 96: train loss = 0.6004, test loss = 0.4714, test accuracy = 0.7541\n",
      "Epoch 97: train loss = 0.6273, test loss = 0.4710, test accuracy = 0.7541\n",
      "Epoch 98: train loss = 0.5517, test loss = 0.4704, test accuracy = 0.7541\n",
      "Epoch 99: train loss = 0.5872, test loss = 0.4700, test accuracy = 0.7541\n",
      "Epoch 100: train loss = 0.6341, test loss = 0.4693, test accuracy = 0.7541\n",
      "Epoch 101: train loss = 0.5220, test loss = 0.4687, test accuracy = 0.7541\n",
      "Epoch 102: train loss = 0.5019, test loss = 0.4682, test accuracy = 0.7541\n",
      "Epoch 103: train loss = 0.6091, test loss = 0.4672, test accuracy = 0.7541\n",
      "Epoch 104: train loss = 0.5108, test loss = 0.4659, test accuracy = 0.7541\n",
      "Epoch 105: train loss = 0.5746, test loss = 0.4649, test accuracy = 0.7541\n",
      "Epoch 106: train loss = 0.5387, test loss = 0.4639, test accuracy = 0.7541\n",
      "Epoch 107: train loss = 0.5566, test loss = 0.4630, test accuracy = 0.7541\n",
      "Epoch 108: train loss = 0.5150, test loss = 0.4624, test accuracy = 0.7541\n",
      "Epoch 109: train loss = 0.5626, test loss = 0.4618, test accuracy = 0.7541\n",
      "Epoch 110: train loss = 0.5322, test loss = 0.4612, test accuracy = 0.7541\n",
      "Epoch 111: train loss = 0.5459, test loss = 0.4606, test accuracy = 0.7541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112: train loss = 0.5981, test loss = 0.4600, test accuracy = 0.7541\n",
      "Epoch 113: train loss = 0.4860, test loss = 0.4592, test accuracy = 0.7541\n",
      "Epoch 114: train loss = 0.4796, test loss = 0.4583, test accuracy = 0.7541\n",
      "Epoch 115: train loss = 0.5922, test loss = 0.4574, test accuracy = 0.7541\n",
      "Epoch 116: train loss = 0.5217, test loss = 0.4564, test accuracy = 0.7705\n",
      "Epoch 117: train loss = 0.5462, test loss = 0.4557, test accuracy = 0.7705\n",
      "Epoch 118: train loss = 0.5419, test loss = 0.4551, test accuracy = 0.7705\n",
      "Epoch 119: train loss = 0.4849, test loss = 0.4544, test accuracy = 0.7705\n",
      "Epoch 120: train loss = 0.4923, test loss = 0.4539, test accuracy = 0.7705\n",
      "Epoch 121: train loss = 0.5855, test loss = 0.4534, test accuracy = 0.7705\n",
      "Epoch 122: train loss = 0.5893, test loss = 0.4530, test accuracy = 0.7705\n",
      "Epoch 123: train loss = 0.5574, test loss = 0.4526, test accuracy = 0.7705\n",
      "Epoch 124: train loss = 0.5468, test loss = 0.4523, test accuracy = 0.7705\n",
      "Epoch 125: train loss = 0.5279, test loss = 0.4520, test accuracy = 0.7705\n",
      "Epoch 126: train loss = 0.6039, test loss = 0.4517, test accuracy = 0.7705\n",
      "Epoch 127: train loss = 0.5610, test loss = 0.4514, test accuracy = 0.7705\n",
      "Epoch 128: train loss = 0.5465, test loss = 0.4512, test accuracy = 0.7705\n",
      "Epoch 129: train loss = 0.5907, test loss = 0.4510, test accuracy = 0.7705\n",
      "Epoch 130: train loss = 0.5497, test loss = 0.4510, test accuracy = 0.7705\n",
      "Epoch 131: train loss = 0.5315, test loss = 0.4508, test accuracy = 0.7541\n",
      "Epoch 132: train loss = 0.5487, test loss = 0.4508, test accuracy = 0.7541\n",
      "Epoch 133: train loss = 0.5539, test loss = 0.4507, test accuracy = 0.7541\n",
      "Epoch 134: train loss = 0.5353, test loss = 0.4506, test accuracy = 0.7541\n",
      "Epoch 135: train loss = 0.5303, test loss = 0.4506, test accuracy = 0.7541\n",
      "Epoch 136: train loss = 0.4932, test loss = 0.4506, test accuracy = 0.7541\n",
      "Epoch 137: train loss = 0.5264, test loss = 0.4503, test accuracy = 0.7541\n",
      "Epoch 138: train loss = 0.5480, test loss = 0.4499, test accuracy = 0.7541\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m evaluate(model, X_test, y_test, loss_fn)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, test loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, test accuracy = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, X, y, optimizer, loss_fn)\u001b[0m\n\u001b[0;32m      5\u001b[0m X_tensor \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mtensor(X\u001b[38;5;241m.\u001b[39mto_numpy(), dtype\u001b[38;5;241m=\u001b[39mth\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# Convert input data to tensor\u001b[39;00m\n\u001b[0;32m      6\u001b[0m y_tensor \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mtensor(y\u001b[38;5;241m.\u001b[39mto_numpy(), dtype\u001b[38;5;241m=\u001b[39mth\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Convert target data to tensor\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tensor\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forward pass to get the model output\u001b[39;00m\n\u001b[0;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, y_tensor)  \u001b[38;5;66;03m# Compute the loss using the loss function\u001b[39;00m\n\u001b[0;32m      9\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Compute the gradients through backpropagation\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\Bachelor\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[13], line 34\u001b[0m, in \u001b[0;36mNAM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Apply each submodule to its corresponding input feature and sum the results\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim):\n\u001b[1;32m---> 34\u001b[0m     output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmodules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Apply sigmoid activation for binary classification\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m th\u001b[38;5;241m.\u001b[39msigmoid(output)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\Bachelor\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\Bachelor\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\Bachelor\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\Bachelor\\lib\\site-packages\\torch\\nn\\modules\\activation.py:517\u001b[0m, in \u001b[0;36mELU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\Bachelor\\lib\\site-packages\\torch\\nn\\functional.py:1548\u001b[0m, in \u001b[0;36melu\u001b[1;34m(input, alpha, inplace)\u001b[0m\n\u001b[0;32m   1546\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39melu_(\u001b[38;5;28minput\u001b[39m, alpha)\n\u001b[0;32m   1547\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1548\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(1000):\n",
    "    train_loss = train(model, X_train, y_train, optimizer, loss_fn)\n",
    "    test_loss, test_accuracy = evaluate(model, X_test, y_test, loss_fn)\n",
    "    print(f'Epoch {epoch + 1}: train loss = {train_loss:.4f}, test loss = {test_loss:.4f}, test accuracy = {test_accuracy:.4f}')\n",
    "\n",
    "print(f'Final test accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
