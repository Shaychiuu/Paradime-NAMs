{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "710e2f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isabellapetrache/miniconda3/envs/Bachelor/lib/python3.10/site-packages/paradime/transforms.py:285: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _entropy(dists: np.ndarray, beta: float) -> float:\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import paradime\n",
    "import paradime.dr\n",
    "import paradime.loss\n",
    "import paradime.routines\n",
    "import paradime.utils\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import shuffle\n",
    "import torchvision\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dd38f98",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sklearn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Download Iris dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m covertype \u001b[38;5;241m=\u001b[39m \u001b[43msklearn\u001b[49m\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mfetch_covtype()\n\u001b[1;32m      4\u001b[0m _, counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(covertype\u001b[38;5;241m.\u001b[39mtarget, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([ \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mcounts[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m covertype\u001b[38;5;241m.\u001b[39mtarget ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sklearn' is not defined"
     ]
    }
   ],
   "source": [
    "# Download Iris dataset\n",
    "covertype = sklearn.datasets.fetch_covtype()\n",
    "\n",
    "_, counts = np.unique(covertype.target, return_counts=True)\n",
    "weights = np.array([ 1/counts[i-1] for i in covertype.target ])\n",
    "\n",
    "indices = list(torch.utils.data.WeightedRandomSampler(weights, 7000))\n",
    "\n",
    "raw_data = covertype.data[indices,:10]\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(raw_data)\n",
    "data = scaler.transform(raw_data)\n",
    "\n",
    "label_to_name = {\n",
    "    1: \"Spruce/fir\",\n",
    "    2: \"Lodgepole pine\",\n",
    "    3: \"Ponderosa pine\",\n",
    "    4: \"Cottonwood/willow\",\n",
    "    5: \"Aspen\",\n",
    "    6: \"Douglas-fir\",\n",
    "    7: \"Krummholz\",\n",
    "}\n",
    "\n",
    "labels = covertype.target[indices]\n",
    "\n",
    "same_label = (np.outer(labels, np.ones_like(labels))\n",
    "    - np.outer(np.ones_like(labels), labels) == 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c124e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "class twoNAMHybrid(th.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, output_dim=2, num_layers=1):\n",
    "        super(twoNAMHybrid, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.submodules = th.nn.ModuleList()\n",
    "        self.alpha = th.nn.Parameter(th.tensor(1.0))\n",
    "\n",
    "        # Create the submodules for each input feature\n",
    "        for i in range(input_dim):\n",
    "            submodule = th.nn.Sequential()\n",
    "            # Add layers to the submodule\n",
    "            for l in range(num_layers):\n",
    "                if l == 0:\n",
    "                    submodule.add_module(f\"linear_{l}\", th.nn.Linear(1, hidden_dim))\n",
    "                else:\n",
    "                    submodule.add_module(f\"linear_{l}\", th.nn.Linear(hidden_dim, hidden_dim))\n",
    "                submodule.add_module(f\"ELU_{l}\", th.nn.ELU())\n",
    "                submodule.add_module(f\"dropout_{l}\", th.nn.Dropout(0.5))\n",
    "\n",
    "            # Add the output layer\n",
    "            submodule.add_module(f\"linear_{num_layers}\", th.nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.submodules.append(submodule)\n",
    "\n",
    "        # Add the final layer\n",
    "        self.emb_layer = th.nn.Linear(input_dim * hidden_dim, output_dim)\n",
    "        self.class_layer = th.nn.Linear(input_dim * hidden_dim, num_classes)\n",
    "\n",
    "    def common_forward(self, x):\n",
    "        # Initialize a list to store the outputs of submodules\n",
    "        output = []\n",
    "        for i in range(self.input_dim):\n",
    "            # Compute the output of the i-th submodule and append it to the list\n",
    "            output.append(self.submodules[i](x[:, i].unsqueeze(1)).squeeze())\n",
    "        # Concatenate the outputs along the first dimension\n",
    "        output = th.cat(output, dim=1)\n",
    "        return output\n",
    "\n",
    "    def embed(self, x):\n",
    "        x = self.common_forward(x)\n",
    "        x = self.emb_layer(x)\n",
    "        return x\n",
    "\n",
    "    def classify(self, x):\n",
    "        x = self.common_forward(x)\n",
    "        x = self.class_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3a6796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(x):\n",
    "    return sklearn.decomposition.PCA(n_components=2).fit_transform(x)\n",
    "\n",
    "derived = paradime.dr.DerivedData(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ced2a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tsne_global_rel = paradime.relations.NeighborBasedPDist(\n",
    "    transform=[\n",
    "        paradime.transforms.PerplexityBasedRescale(\n",
    "            perplexity=200, bracket=[0.001, 1000]\n",
    "        ),\n",
    "        paradime.transforms.Symmetrize(),\n",
    "        paradime.transforms.Normalize(),\n",
    "    ]\n",
    ")\n",
    "\n",
    ":\n",
    "tsne_batch_rel = paradime.relations.DifferentiablePDist(\n",
    "    transform=[\n",
    "        paradime.transforms.StudentTTransform(alpha=1.0),\n",
    "        paradime.transforms.Normalize(),\n",
    "        paradime.transforms.ToSquareTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "class TripletLoss(paradime.loss.Loss):\n",
    "    \"\"\"Triplet loss for supervised DR.\n",
    "\n",
    "    To be used with negative edge sampling with sampling rate 1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=1.0, name=None):\n",
    "        super().__init__(name)\n",
    "\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, model, global_relations,  batch_relations, batch, device):\n",
    "\n",
    "        data = batch['from_to_data'].to(device)\n",
    "        # data consists of [[a0, a0, a1, a1, ...], [p0, n0, p1, n1, ...]]\n",
    "\n",
    "        anchor = model(data[0,::2])\n",
    "        positive = model(data[1,::2])\n",
    "        negative = model(data[1,1::2])\n",
    "\n",
    "        loss = torch.nn.TripletMarginLoss(margin=self.margin)\n",
    "\n",
    "        return loss(anchor, positive, negative)\n",
    "    \n",
    "new_losses = {\n",
    "    \"init\": paradime.loss.PositionLoss(position_key=\"pca\"),\n",
    "    \"embedding\": paradime.loss.RelationLoss(\n",
    "        loss_function=paradime.loss.kullback_leibler_div,\n",
    "        global_relation_key=\"tsne\",\n",
    "    ),\n",
    "    \"triplet\": TripletLoss(),\n",
    "}\n",
    "\n",
    "tsne_init = paradime.dr.TrainingPhase(\n",
    "    name=\"pca_init\",\n",
    "    loss_keys=[\"init\"],\n",
    "    batch_size=500,\n",
    "    epochs=10,\n",
    "    learning_rate=0.01,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e42f9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "super_tsne = paradime.dr.ParametricDR(\n",
    "    model=twoNAMHybrid(\n",
    "        input_dim=2, hidden_dim=100, num_classes=10, output_dim=2,\n",
    "    )\n",
    "    global_relations={\n",
    "        \"tsne\": tsne_global_rel,\n",
    "        \"same_label\": paradime.relations.Precomputed(same_label),\n",
    "    },\n",
    "    batch_relations=tsne_batch_rel,\n",
    "    losses=new_losses,\n",
    "    derived_data={\"pca\": derived},\n",
    "    use_cuda=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "###########\n",
    "\n",
    "super_tsne.add_training_phase(tsne_init)\n",
    "super_tsne.add_training_phase(\n",
    "    name=\"embedding\",\n",
    "    loss_keys=[\"embedding\", \"triplet\"],\n",
    "    loss_weights=[700, 1],\n",
    "    sampling=\"negative_edge\",\n",
    "    neg_sampling_rate=1,\n",
    "    edge_rel_key=\"same_label\",\n",
    "    batch_size=300,\n",
    "    epochs=40,\n",
    "    learning_rate=0.02,\n",
    "    report_interval=2,\n",
    ")\n",
    "super_tsne.train(data)\n",
    "\n",
    "#embeddings.append(hybrid_tsne.apply(iris_subset, \"embed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4971dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "paradime.utils.plotting.scatterplot(\n",
    "    super_tsne.apply(data),\n",
    "    labels=[label_to_name[i] for i in covertype.target[indices]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7479bb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 500\n",
    "\n",
    "# Initialize an empty tensor to store all embeddings\n",
    "all_embeddings = th.Tensor()\n",
    "\n",
    "# Compute embeddings for each batch\n",
    "for i in range(0, len(iris_data), batch_size):\n",
    "    batch_data = iris_data[i : i + batch_size]\n",
    "    batch_embeddings = hybrid_tsne.apply(batch_data, \"embed\")\n",
    "    all_embeddings = th.cat([all_embeddings, batch_embeddings])\n",
    "\n",
    "# Move all embeddings to cpu\n",
    "all_embeddings = all_embeddings.cpu()\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "ax = fig.add_subplot(1, 3, 1)\n",
    "\n",
    "paradime.utils.plotting.scatterplot(\n",
    "    all_embeddings,\n",
    "    labels=iris_targets,\n",
    "    ax=ax,\n",
    "    legend=True,\n",
    "    legend_options={\"loc\": 3},\n",
    ")\n",
    "\n",
    "ax.set_title(f\"t-SNE visualization of Iris dataset - additional Datapoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f74155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a data point\n",
    "idx = 0\n",
    "data_point = iris_data[idx]\n",
    "\n",
    "# Ensure the data point has the correct dimensions\n",
    "if len(data_point.shape) == 1:\n",
    "    data_point = data_point.view(1, -1)\n",
    "\n",
    "# Move the data to the GPU\n",
    "data_point = data_point.to('cuda')\n",
    "\n",
    "# Get the output of each submodule and compute the mean\n",
    "output = []\n",
    "for i, submodule in enumerate(hybrid_tsne.model.submodules):\n",
    "    submodule_output = submodule(data_point[:, i].unsqueeze(1))\n",
    "    # Ensure the output is a 2D tensor\n",
    "    if len(submodule_output.shape) == 1:\n",
    "        submodule_output = submodule_output.view(1, -1)\n",
    "    # Compute the mean of the output vector\n",
    "    mean_contribution = th.mean(submodule_output)\n",
    "    output.append(mean_contribution)\n",
    "\n",
    "# Convert the list to a tensor\n",
    "output = th.stack(output)\n",
    "\n",
    "# Move the output back to the CPU for plotting\n",
    "output = output.to('cpu')\n",
    "\n",
    "# Create a figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the outputs of the submodules\n",
    "ax.bar(range(hybrid_tsne.model.input_dim), output.detach().numpy())\n",
    "\n",
    "# Set the title and labels\n",
    "ax.set_title('Principal Component contributions for data point {}'.format(idx))\n",
    "ax.set_xlabel('Principal Component')\n",
    "ax.set_ylabel('Contribution')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950d6590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
