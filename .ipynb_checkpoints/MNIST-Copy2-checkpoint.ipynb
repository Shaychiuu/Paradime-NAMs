{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "710e2f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isabellapetrache/miniconda3/envs/Bachelor/lib/python3.10/site-packages/paradime/transforms.py:285: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _entropy(dists: np.ndarray, beta: float) -> float:\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import paradime\n",
    "import paradime.dr\n",
    "import paradime.loss\n",
    "import paradime.routines\n",
    "import paradime.utils\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets\n",
    "import torchvision\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dd38f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = torchvision.datasets.MNIST(\n",
    "    '../data',\n",
    "    train=True,\n",
    "    download=True,\n",
    ")\n",
    "mnist_data = mnist.data.reshape(-1, 28*28) / 255.\n",
    "num_items = 5000\n",
    "\n",
    "mnist_subset = mnist_data[:num_items]\n",
    "target_subset = mnist.targets[:num_items]\n",
    "\n",
    "th.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c124e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "class twoNAMHybrid(th.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, output_dim=2, num_layers=1):\n",
    "        super(twoNAMHybrid, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.submodules = th.nn.ModuleList()\n",
    "        self.alpha = th.nn.Parameter(th.tensor(1.0))\n",
    "\n",
    "        # Create the submodules for each input feature\n",
    "        for i in range(input_dim):\n",
    "            submodule = th.nn.Sequential()\n",
    "            # Add layers to the submodule\n",
    "            for l in range(num_layers):\n",
    "                if l == 0:\n",
    "                    submodule.add_module(f\"linear_{l}\", th.nn.Linear(1, hidden_dim))\n",
    "                else:\n",
    "                    submodule.add_module(f\"linear_{l}\", th.nn.Linear(hidden_dim, hidden_dim))\n",
    "                submodule.add_module(f\"ELU_{l}\", th.nn.ELU())\n",
    "                submodule.add_module(f\"dropout_{l}\", th.nn.Dropout(0.5))\n",
    "\n",
    "            # Add the output layer\n",
    "            submodule.add_module(f\"linear_{num_layers}\", th.nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.submodules.append(submodule)\n",
    "\n",
    "        # Add the final layer\n",
    "        self.emb_layer = th.nn.Linear(input_dim * hidden_dim, output_dim)\n",
    "        self.class_layer = th.nn.Linear(input_dim * hidden_dim, num_classes)\n",
    "\n",
    "    def common_forward(self, x):\n",
    "        # Initialize a list to store the outputs of submodules\n",
    "        output = []\n",
    "        for i in range(self.input_dim):\n",
    "            # Compute the output of the i-th submodule and append it to the list\n",
    "            output.append(self.submodules[i](x[:, i].unsqueeze(1)).squeeze())\n",
    "        # Concatenate the outputs along the first dimension\n",
    "        output = th.cat(output, dim=1)\n",
    "        return output\n",
    "\n",
    "    def embed(self, x):\n",
    "        x = self.common_forward(x)\n",
    "        x = self.emb_layer(x)\n",
    "        return x\n",
    "\n",
    "    def classify(self, x):\n",
    "        x = self.common_forward(x)\n",
    "        x = self.class_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f3a6796",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim_values = [50, 100, 200]\n",
    "num_layers_values = [1, 2, 3]\n",
    "\n",
    "best_score = float('inf')\n",
    "best_hidden_dim = None\n",
    "best_num_layers = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ced2a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = paradime.routines.ParametricTSNE(in_dim=28*28)\n",
    "global_rel = tsne.global_relations\n",
    "batch_rel = tsne.batch_relations\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "losses = {\n",
    "    \"embedding\": paradime.loss.RelationLoss(\n",
    "        loss_function=paradime.loss.kullback_leibler_div\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Define a single training phase for the t-SNE embedding\n",
    "tsne_main = paradime.dr.TrainingPhase(\n",
    "    name=\"embedding\",\n",
    "    loss_keys=[\"embedding\"],\n",
    "    batch_size=500,\n",
    "    epochs=40,\n",
    "    learning_rate=0.02,\n",
    "    report_interval=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e42f9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-02 20:13:27,112: Initializing training dataset.\n",
      "2023-10-02 20:13:27,114: Computing global relations 'rel'.\n",
      "2023-10-02 20:13:27,115: Indexing nearest neighbors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-02 20:13:35,673: Calculating probabilities.\n",
      "2023-10-02 20:13:36,035: Beginning training phase 'embedding'.\n",
      "2023-10-02 20:13:43,158: Loss after epoch 0: 0.096550771035254\n",
      "2023-10-02 20:13:57,227: Loss after epoch 2: 0.11389419995248318\n"
     ]
    }
   ],
   "source": [
    "hybrid_tsne = paradime.dr.ParametricDR(\n",
    "    model=twoNAMHybrid(\n",
    "        input_dim=28 * 28, hidden_dim=100, num_classes=10, output_dim=2,\n",
    "    ),\n",
    "    global_relations=global_rel,\n",
    "    batch_relations=batch_rel,\n",
    "    losses=losses,\n",
    "    use_cuda=False,\n",
    "    verbose=True,\n",
    ")\n",
    "hybrid_tsne.add_training_phase(tsne_main)\n",
    "\n",
    "hybrid_tsne.train({\n",
    "    \"main\": mnist_subset,\n",
    "    \"labels\": target_subset,\n",
    "})\n",
    "\n",
    "embeddings.append(hybrid_tsne.apply(mnist_subset, \"embed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4971dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "ax = fig.add_subplot(1, 3, 1)\n",
    "    \n",
    "# Use Paradime's scatterplot utility to plot the embedding with class labels as colors\n",
    "paradime.utils.plotting.scatterplot(\n",
    "    embeddings[0],\n",
    "    labels=target_subset,\n",
    "    ax=ax,\n",
    "    legend=True,\n",
    "    legend_options={\"loc\": 3},\n",
    ")\n",
    "ax.set_title(f\"t-SNE visualization of MNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b24af2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 500\n",
    "\n",
    "# Initialize an empty tensor to store all embeddings\n",
    "all_embeddings = th.Tensor()\n",
    "\n",
    "# Compute embeddings for each batch\n",
    "for i in range(0, len(mnist_data), batch_size):\n",
    "    batch_data = mnist_data[i : i + batch_size]\n",
    "    batch_embeddings = hybrid_tsne.apply(batch_data, \"embed\")\n",
    "    all_embeddings = th.cat([all_embeddings, batch_embeddings])\n",
    "\n",
    "# Move all embeddings to cpu\n",
    "all_embeddings = all_embeddings.cpu()\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "ax = fig.add_subplot(1, 3, 1)\n",
    "\n",
    "paradime.utils.plotting.scatterplot(\n",
    "    all_embeddings,\n",
    "    labels=mnist.targets,\n",
    "    ax=ax,\n",
    "    legend=True,\n",
    "    legend_options={\"loc\": 3},\n",
    ")\n",
    "\n",
    "ax.set_title(f\"t-SNE visualization of MNIST - additional Datapoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f74155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a data point\n",
    "idx = 10\n",
    "data_point = mnist_subset[idx]\n",
    "\n",
    "# Ensure the data point has the correct dimensions\n",
    "if len(data_point.shape) == 1:\n",
    "    data_point = data_point.view(1, -1)\n",
    "\n",
    "# Move the data to the GPU\n",
    "data_point = data_point.to('cuda')\n",
    "\n",
    "# Get the output of each submodule and compute the mean\n",
    "output = []\n",
    "for i, submodule in enumerate(hybrid_tsne.model.submodules):\n",
    "    submodule_output = submodule(data_point[:, i].unsqueeze(1))\n",
    "    # Ensure the output is a 2D tensor\n",
    "    if len(submodule_output.shape) == 1:\n",
    "        submodule_output = submodule_output.view(1, -1)\n",
    "    # Compute the mean of the output vector\n",
    "    mean_contribution = th.mean(submodule_output)\n",
    "    output.append(mean_contribution)\n",
    "\n",
    "# Convert the list to a tensor\n",
    "output = th.stack(output)\n",
    "\n",
    "# Move the output back to the CPU for plotting\n",
    "output = output.to('cpu')\n",
    "\n",
    "# Convert the output tensor to a 2D image\n",
    "output_2d = output.view(28, 28)\n",
    "\n",
    "# Create a figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Create a heatmap\n",
    "cax = ax.matshow(output_2d.detach().numpy(), cmap='viridis')\n",
    "\n",
    "# Add a colorbar\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set the title\n",
    "ax.set_title('Pixel importance for data point {}'.format(idx))\n",
    "\n",
    "# Hide the axes labels\n",
    "ax.axis('off')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507504ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
